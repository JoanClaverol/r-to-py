---
title: "Predict housing prices in Austin TX with tidymodels and xgboost"
output: github_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = FALSE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(reticulate)
library(tidyverse, quietly = TRUE)
theme_set(theme_minimal())
update_geom_defaults("rect", list(fill = "midnightblue", alpha = 0.8))
```

Original notebook https://juliasilge.com/blog/austin-housing/.

# Explore data

```{r}
# r
library(tidyverse)
train_raw <- read_csv("train.csv")

train_raw %>%
  count(priceRange)
```

```{python}
# py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

train_raw = pd.read_csv("train.csv")

train_raw.groupby("priceRange")["priceRange"].count()
```


## Price distribution

```{r}
# r
price_plot <-
  train_raw %>%
  mutate(priceRange = parse_number(priceRange)) %>%
  ggplot(aes(longitude, latitude, z = priceRange)) +
  stat_summary_hex(alpha = 0.8, bins = 50) +
  scale_fill_viridis_c() +
  labs(
    fill = "mean",
    title = "Price"
  )

price_plot
```

```{python}
# py
def clean_price_range(num): 
  return int(num.split("-")[-1].replace("+",""))

p_df = train_raw.copy()
p_df["priceRange"] = (
  train_raw["priceRange"]
  .apply(lambda x: clean_price_range(x))
  )

xmin = p_df["longitude"].min() - .01
xmax = p_df["longitude"].max() + .01
ymin = p_df["latitude"].min() - .01
ymax = p_df["latitude"].max() + .01

plt.cla()
fig, ax = plt.subplots(ncols=1, sharey=True, figsize=(8, 5))

hb = plt.hexbin(
  p_df["longitude"], p_df["latitude"], C=p_df["priceRange"], 
  gridsize=50, cmap='viridis', 
  alpha=.8
  )
_ = ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))
_ = ax.set_title("Price")
cb = fig.colorbar(hb, ax=ax)

# plt.show()
```

## More information with charts


```{r}
# r
plot_austin <- function(var, title) {
  train_raw %>%
    ggplot(aes(longitude, latitude, z = {{ var }})) +
    stat_summary_hex(alpha = 0.8, bins = 50) +
    scale_fill_viridis_c() +
    labs(
      fill = "mean",
      title = title
    )
}

library(patchwork)
(price_plot + plot_austin(avgSchoolRating, "School rating")) /
  (plot_austin(yearBuilt, "Year built") + plot_austin(log(lotSizeSqFt), "Lot size (log)"))
```

```{python}
# py
xmin = p_df["longitude"].min() - .01
xmax = p_df["longitude"].max() + .01
ymin = p_df["latitude"].min() - .01
ymax = p_df["latitude"].max() + .01

plt.cla()
fig, axs = plt.subplots(ncols=2, nrows=2, sharey=True, figsize=(8, 5))
fig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)

col_names = ["priceRange","avgSchoolRating","yearBuilt","lotSizeSqFt"]
col_pos = 0
for col in range(2): 
  for row in range(2):
    ax = axs[col,row]
    hb = ax.hexbin(
      p_df["longitude"], p_df["latitude"], C=p_df[col_names[col_pos]], 
      gridsize=50, cmap='viridis', 
      alpha=.8
      )
    _ = ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))
    _ = ax.set_title(col_names[col_pos])
    cb = fig.colorbar(hb, ax=ax)
    col_pos += 1

# plt.show()
```

# Finding words related to price

## Tidy data

```{r}
# r
library(tidytext)

austin_tidy <-
  train_raw %>%
  mutate(priceRange = parse_number(priceRange) + 100000) %>%
  unnest_tokens(word, description) %>%
  anti_join(get_stopwords(), by="word")

austin_tidy %>%
  count(word, sort = TRUE) %>% 
  head()
```

```{python}
# py
austin_tidy = (
train_raw
  .assign(priceRange = lambda x: (
    x["priceRange"].apply(lambda x: clean_price_range(x))) + 100000
  )
  .copy()
)
```


## Words frequency

```{r}
# r
top_words <-
  austin_tidy %>%
  count(word, sort = TRUE) %>%
  filter(!word %in% as.character(1:5)) %>%
  slice_max(n, n = 100) %>%
  pull(word)


word_freqs <-
  austin_tidy %>%
  count(word, priceRange) %>%
  complete(word, priceRange, fill = list(n = 0)) %>%
  group_by(priceRange) %>%
  mutate(
    price_total = sum(n),
    proportion = n / price_total
  ) %>%
  ungroup() %>%
  filter(word %in% top_words)


word_freqs %>% head()
```

## Create our model

```{r}
word_mods <-
  word_freqs %>%
  nest(data = c(priceRange, n, price_total, proportion)) %>%
  mutate(
    model = map(data, ~ glm(cbind(n, price_total) ~ priceRange, ., family = "binomial")),
    model = map(model, tidy)
  ) %>%
  unnest(model) %>%
  filter(term == "priceRange") %>%
  mutate(p.value = p.adjust(p.value)) %>%
  arrange(-estimate)

word_mods %>% head()
```

## Relationship between p-value and effect size for these word

```{r}
# r
library(ggrepel)

word_mods %>%
  ggplot(aes(estimate, p.value)) +
  geom_vline(xintercept = 0, lty = 2, alpha = 0.7, color = "gray50") +
  geom_point(color = "midnightblue", alpha = 0.8, size = 2.5) +
  scale_y_log10() +
  geom_text_repel(aes(label = word))
```

